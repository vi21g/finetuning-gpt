# Fine-tuning трансформера для генерации заголовков

[Ссылка на колаб на случай, если не работает ipynb в репозитории](https://colab.research.google.com/drive/1D84MFWlUWw_WFWee-OPCPWsoZcXoaghH?usp=sharing)

## Описание проекта
Этот проект демонстрирует процесс дообучения (fine-tuning) GPT-подобной модели (rugpt3medium от Сбера) для задачи генерации заголовков к текстам новостей. Модель обучается по принципу "текст → заголовок" с использованием специальных токенов формата.

## Формат данных
Для обучения используется специальный формат с разделителями:
```
[TEXT] текст статьи [TITLE] заголовок статьи
```
Для инференса (генерации) подаётся:
```
[TEXT] текст статьи [TITLE]
```

## Техническая реализация

### Используемые технологии
- Модель: `sberbank-ai/rugpt3medium_based_on_gpt2` (русскоязычная GPT-3 от Сбера)
- Фреймворк: HuggingFace Transformers
- Ускорение: CUDA (PyTorch)

### Основные этапы:
1. **Подготовка данных**:
   - Загрузка датасета русскоязычных новостей
   - Преобразование в формат `[TEXT]... [TITLE]...`
   - Конвертация в HuggingFace Dataset

2. **Настройка модели**:
   ```python
   # Добавление специальных токенов
   special_tokens = ["[TEXT]", "[TITLE]"]
   tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
   model.resize_token_embeddings(len(tokenizer))
   ```

3. **Параметры обучения**:
   ```python
   training_args = TrainingArguments(
       output_dir="./finetuned",
       num_train_epochs=1,  # Учебный вариант - см. рекомендации
       per_device_train_batch_size=2,
       gradient_accumulation_steps=4,
       learning_rate=3e-5,
       warmup_steps=100,
       fp16=True  # Использование GPU
   )
   ```

## Рекомендации по улучшению

1. **Увеличение числа эпох**:
   - В учебных целях использовалась 1 эпоха
   - Для реальных задач рекомендуется 3-5 эпох
   - Следить за переобучением (использовать валидационный набор)

2. **Оптимизация гиперпараметров**:
   - Постепенное уменьшение learning rate
   - Подбор оптимального размера батча
   - Использование ранней остановки

3. **Улучшение данных**:
   - Увеличение размера датасета
   - Более тщательная предобработка текстов
   - Балансировка по тематикам/длине текстов

## Требования
- Python 3.7+
- PyTorch с поддержкой CUDA
- Библиотеки: transformers, datasets, pandas

## Выводы
Проект подтвердил принципиальную возможность эффективного дообучения GPT-модели для задачи генерации заголовков. Полученная модель может служить базой для дальнейшего улучшения с учетом приведенных рекомендаций.
